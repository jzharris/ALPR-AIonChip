C:\Users\zharr\Git\UCLA\AIOnChip Projects\FinalProject\sandbox\YOLOv2\keras-yolo2>python train.py -c config_lp_seg_mobilenet.json
Using TensorFlow backend.
Seen labels:     {'plate': 1856}
Given labels:    ['plate']
Overlap labels:  {'plate'}
2019-03-08 20:30:48.148530: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-03-08 20:30:48.727594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.835
pciBusID: 0000:0b:00.0
totalMemory: 8.00GiB freeMemory: 6.61GiB
2019-03-08 20:30:48.733154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2019-03-08 20:30:49.645663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-08 20:30:49.648890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0
2019-03-08 20:30:49.655471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N
2019-03-08 20:30:49.657560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6367 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:0b:00.0, compu
te capability: 6.1)
(14, 14)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_3 (InputLayer)            (None, 448, 448, 3)  0
__________________________________________________________________________________________________
mobilenet_1.00_224 (Model)      multiple             3228864     input_3[0][0]
__________________________________________________________________________________________________
DetectionLayer (Conv2D)         (None, 14, 14, 30)   30750       mobilenet_1.00_224[1][0]
__________________________________________________________________________________________________
reshape_1 (Reshape)             (None, 14, 14, 5, 6) 0           DetectionLayer[0][0]
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 1, 1, 1, 1, 4 0
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, 14, 14, 5, 6) 0           reshape_1[0][0]
                                                                 input_2[0][0]
==================================================================================================
Total params: 3,259,614
Trainable params: 3,237,726
Non-trainable params: 21,888
__________________________________________________________________________________________________
it 0
Loading pre-trained weights in lp_seg_mobilenet.h5
Evaluating pre-trained network
plate 0.9930
mAP: 0.9930
Pruning parameters one layer at a time...
Checking that the pruned weights were not modified...
>>>     0 of the 637016 weights that have been pruned are NONzero (should be 0)
>>>     637016 of the 3259614 total weights have been pruned (19.542682% of original, should be 20.000000%)
====================
Saving pruned weights for next iteration...
Evaluating pruned network, before train step:
plate 0.9672
mAP: 0.9672
it 1
Loading pruned weights in lp_seg_mobilenet_pruned.h5
Checking that the pruned weights were not modified...
>>>     0 of the 637016 weights that have been pruned are NONzero (should be 0)
>>>     637016 of the 3259614 total weights have been pruned (19.542682% of original, should be 20.000000%)
Epoch 1/1
2019-03-08 20:32:32.886885: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.21GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more me
mory were available.
2019-03-08 20:32:32.921216: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.15GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more me
mory were available.
2019-03-08 20:32:32.938558: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more me
mory were available.
2019-03-08 20:32:32.970515: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.26GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more me
mory were available.
2019-03-08 20:32:32.982893: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more me
mory were available.
2019-03-08 20:32:33.000959: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more me
mory were available.
2019-03-08 20:32:33.009486: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.17GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more me
mory were available.
2019-03-08 20:32:33.027492: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.15GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more me
mory were available.
2019-03-08 20:32:33.039633: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.21GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more me
mory were available.
2019-03-08 20:32:33.122741: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.22GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more me
mory were available.
704/704 [==============================] - 385s 547ms/step - loss: 0.0129 - val_loss: 0.0154

Epoch 00001: val_loss improved from inf to 0.01538, saving model to lp_seg_mobilenet_last_unpruned.h5
plate 0.9962
mAP: 0.9962
Checking that the pruned weights were not modified...
>>>     0 of the 637016 weights that have been pruned are NONzero (should be 0)
>>>     637016 of the 3259614 total weights have been pruned (19.542682% of original, should be 20.000000%)
Pruning parameters one layer at a time...
Checking that the pruned weights were not modified...
>>>     0 of the 1146636 weights that have been pruned are NONzero (should be 0)
>>>     1146636 of the 3259614 total weights have been pruned (35.177049% of original, should be 36.000000%)
====================
Saving pruned weights for next iteration...
Evaluating pruned network, before train step:
plate 0.9600
mAP: 0.9600
it 2
Loading pruned weights in lp_seg_mobilenet_pruned.h5
Checking that the pruned weights were not modified...
>>>     0 of the 1146636 weights that have been pruned are NONzero (should be 0)
>>>     1146636 of the 3259614 total weights have been pruned (35.177049% of original, should be 36.000000%)
Epoch 1/1
704/704 [==============================] - 384s 546ms/step - loss: 0.0129 - val_loss: 0.0168

Epoch 00001: val_loss improved from inf to 0.01677, saving model to lp_seg_mobilenet_last_unpruned.h5
plate 0.9962
mAP: 0.9962
Checking that the pruned weights were not modified...
>>>     0 of the 1146636 weights that have been pruned are NONzero (should be 0)
>>>     1146636 of the 3259614 total weights have been pruned (35.177049% of original, should be 36.000000%)
Pruning parameters one layer at a time...
Checking that the pruned weights were not modified...
>>>     0 of the 1554328 weights that have been pruned are NONzero (should be 0)
>>>     1554328 of the 3259614 total weights have been pruned (47.684419% of original, should be 48.800000%)
====================
Saving pruned weights for next iteration...
Evaluating pruned network, before train step:
plate 0.5552
mAP: 0.5552
it 3
Loading pruned weights in lp_seg_mobilenet_pruned.h5
Checking that the pruned weights were not modified...
>>>     0 of the 1554328 weights that have been pruned are NONzero (should be 0)
>>>     1554328 of the 3259614 total weights have been pruned (47.684419% of original, should be 48.800000%)
Epoch 1/1
704/704 [==============================] - 384s 545ms/step - loss: 0.0144 - val_loss: 0.0170

Epoch 00001: val_loss improved from inf to 0.01698, saving model to lp_seg_mobilenet_last_unpruned.h5
plate 0.9924
mAP: 0.9924
Checking that the pruned weights were not modified...
>>>     0 of the 1554328 weights that have been pruned are NONzero (should be 0)
>>>     1554328 of the 3259614 total weights have been pruned (47.684419% of original, should be 48.800000%)
Pruning parameters one layer at a time...
Checking that the pruned weights were not modified...
>>>     0 of the 1880486 weights that have been pruned are NONzero (should be 0)
>>>     1880486 of the 3259614 total weights have been pruned (57.690450% of original, should be 59.040000%)
====================
Saving pruned weights for next iteration...
Evaluating pruned network, before train step:
plate 0.0097
mAP: 0.0097
it 4
Loading pruned weights in lp_seg_mobilenet_pruned.h5
Checking that the pruned weights were not modified...
>>>     0 of the 1880486 weights that have been pruned are NONzero (should be 0)
>>>     1880486 of the 3259614 total weights have been pruned (57.690450% of original, should be 59.040000%)
Epoch 1/1
704/704 [==============================] - 386s 548ms/step - loss: 0.0169 - val_loss: 0.0170

Epoch 00001: val_loss improved from inf to 0.01695, saving model to lp_seg_mobilenet_last_unpruned.h5
plate 0.9926
mAP: 0.9926
Checking that the pruned weights were not modified...
>>>     0 of the 1880486 weights that have been pruned are NONzero (should be 0)
>>>     1880486 of the 3259614 total weights have been pruned (57.690450% of original, should be 59.040000%)
Pruning parameters one layer at a time...
Checking that the pruned weights were not modified...
>>>     0 of the 2141408 weights that have been pruned are NONzero (should be 0)
>>>     2141408 of the 3259614 total weights have been pruned (65.695141% of original, should be 67.232000%)
====================
Saving pruned weights for next iteration...
Evaluating pruned network, before train step:
plate 0.0000
mAP: 0.0000
it 5
Loading pruned weights in lp_seg_mobilenet_pruned.h5
Checking that the pruned weights were not modified...
>>>     0 of the 2141408 weights that have been pruned are NONzero (should be 0)
>>>     2141408 of the 3259614 total weights have been pruned (65.695141% of original, should be 67.232000%)
Epoch 1/1
704/704 [==============================] - 377s 535ms/step - loss: 0.0216 - val_loss: 0.0199

Epoch 00001: val_loss improved from inf to 0.01994, saving model to lp_seg_mobilenet_last_unpruned.h5
plate 0.9944
mAP: 0.9944
Checking that the pruned weights were not modified...
>>>     0 of the 2141408 weights that have been pruned are NONzero (should be 0)
>>>     2141408 of the 3259614 total weights have been pruned (65.695141% of original, should be 67.232000%)
Pruning parameters one layer at a time...
Checking that the pruned weights were not modified...
>>>     0 of the 2350150 weights that have been pruned are NONzero (should be 0)
>>>     2350150 of the 3259614 total weights have been pruned (72.099028% of original, should be 73.785600%)
====================
Saving pruned weights for next iteration...
Evaluating pruned network, before train step:
plate 0.0000
mAP: 0.0000
it 6
Loading pruned weights in lp_seg_mobilenet_pruned.h5
Checking that the pruned weights were not modified...
>>>     0 of the 2350150 weights that have been pruned are NONzero (should be 0)
>>>     2350150 of the 3259614 total weights have been pruned (72.099028% of original, should be 73.785600%)
Epoch 1/1
704/704 [==============================] - 420s 597ms/step - loss: 0.0293 - val_loss: 0.0240

Epoch 00001: val_loss improved from inf to 0.02401, saving model to lp_seg_mobilenet_last_unpruned.h5
plate 0.9842
mAP: 0.9842
Checking that the pruned weights were not modified...
>>>     0 of the 2350150 weights that have been pruned are NONzero (should be 0)
>>>     2350150 of the 3259614 total weights have been pruned (72.099028% of original, should be 73.785600%)
Pruning parameters one layer at a time...
Checking that the pruned weights were not modified...
>>>     0 of the 2517144 weights that have been pruned are NONzero (should be 0)
>>>     2517144 of the 3259614 total weights have been pruned (77.222150% of original, should be 79.028480%)
====================
Saving pruned weights for next iteration...
Evaluating pruned network, before train step:
plate 0.0000
mAP: 0.0000
it 7
Loading pruned weights in lp_seg_mobilenet_pruned.h5
Checking that the pruned weights were not modified...
>>>     0 of the 2517144 weights that have been pruned are NONzero (should be 0)
>>>     2517144 of the 3259614 total weights have been pruned (77.222150% of original, should be 79.028480%)
Epoch 1/1
704/704 [==============================] - 380s 540ms/step - loss: 0.0428 - val_loss: 0.0284

Epoch 00001: val_loss improved from inf to 0.02841, saving model to lp_seg_mobilenet_last_unpruned.h5
C:\Users\zharr\Git\UCLA\AIOnChip Projects\FinalProject\sandbox\YOLOv2\keras-yolo2\utils.py:205: RuntimeWarning: overflow encountered in exp
  return 1. / (1. + np.exp(-x))
plate 0.9636
mAP: 0.9636
Checking that the pruned weights were not modified...
>>>     0 of the 2517144 weights that have been pruned are NONzero (should be 0)
>>>     2517144 of the 3259614 total weights have been pruned (77.222150% of original, should be 79.028480%)
Pruning parameters one layer at a time...
Checking that the pruned weights were not modified...
>>>     0 of the 2650730 weights that have been pruned are NONzero (should be 0)
>>>     2650730 of the 3259614 total weights have been pruned (81.320365% of original, should be 83.222784%)
====================
Saving pruned weights for next iteration...
Evaluating pruned network, before train step:
plate 0.0000
mAP: 0.0000
it 8
Loading pruned weights in lp_seg_mobilenet_pruned.h5
Checking that the pruned weights were not modified...
>>>     0 of the 2650730 weights that have been pruned are NONzero (should be 0)
>>>     2650730 of the 3259614 total weights have been pruned (81.320365% of original, should be 83.222784%)
Epoch 1/1
704/704 [==============================] - 382s 543ms/step - loss: 0.0630 - val_loss: 0.0393

Epoch 00001: val_loss improved from inf to 0.03930, saving model to lp_seg_mobilenet_last_unpruned.h5
plate 0.9343
mAP: 0.9343
Checking that the pruned weights were not modified...
>>>     0 of the 2650730 weights that have been pruned are NONzero (should be 0)
>>>     2650730 of the 3259614 total weights have been pruned (81.320365% of original, should be 83.222784%)
Pruning parameters one layer at a time...
Checking that the pruned weights were not modified...
>>>     0 of the 2757602 weights that have been pruned are NONzero (should be 0)
>>>     2757602 of the 3259614 total weights have been pruned (84.599035% of original, should be 86.578227%)
====================
Saving pruned weights for next iteration...
Evaluating pruned network, before train step:
plate 0.0000
mAP: 0.0000
