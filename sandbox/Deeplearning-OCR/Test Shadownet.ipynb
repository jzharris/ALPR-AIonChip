{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as ops\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from local_utils import data_utils\n",
    "from crnn_model import crnn_model\n",
    "from global_configuration import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_shadownet(dataset_dir, weights_path, is_vis=False, is_recursive=True):\n",
    "    # Initialize the record decoder\n",
    "    decoder = data_utils.TextFeatureIO().reader\n",
    "    images_t, labels_t, imagenames_t = decoder.read_features(\n",
    "        ops.join(dataset_dir, 'test_feature.tfrecords'), num_epochs=None)\n",
    "    if not is_recursive:\n",
    "        images_sh, labels_sh, imagenames_sh = tf.train.shuffle_batch(tensors=[images_t, labels_t, imagenames_t],\n",
    "                                                                     batch_size=32, capacity=1000+32*2,\n",
    "                                                                     min_after_dequeue=2, num_threads=4)\n",
    "    else:\n",
    "        images_sh, labels_sh, imagenames_sh = tf.train.batch(tensors=[images_t, labels_t, imagenames_t],\n",
    "                                                             batch_size=32, capacity=1000 + 32 * 2, num_threads=4)\n",
    "\n",
    "    images_sh = tf.cast(x=images_sh, dtype=tf.float32)\n",
    "\n",
    "    # build shadownet\n",
    "    net = crnn_model.ShadowNet(phase='Test', hidden_nums=256, layers_nums=2, seq_length=25, num_classes=37)\n",
    "\n",
    "    with tf.variable_scope('shadow'):\n",
    "        net_out = net.build_shadownet(inputdata=images_sh)\n",
    "\n",
    "    decoded, _ = tf.nn.ctc_beam_search_decoder(net_out, 25 * np.ones(32), merge_repeated=False)\n",
    "\n",
    "    # config tf session\n",
    "    sess_config = tf.ConfigProto()\n",
    "    sess_config.gpu_options.per_process_gpu_memory_fraction = config.cfg.TRAIN.GPU_MEMORY_FRACTION\n",
    "    sess_config.gpu_options.allow_growth = config.cfg.TRAIN.TF_ALLOW_GROWTH\n",
    "\n",
    "    # config tf saver\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess = tf.Session(config=sess_config)\n",
    "\n",
    "    test_sample_count = 0\n",
    "    for record in tf.python_io.tf_record_iterator(ops.join(dataset_dir, 'test_feature.tfrecords')):\n",
    "        test_sample_count += 1\n",
    "    loops_nums = int(math.ceil(test_sample_count / 32))\n",
    "    # loops_nums = 100\n",
    "\n",
    "    with sess.as_default():\n",
    "\n",
    "        # restore the model weights\n",
    "        saver.restore(sess=sess, save_path=weights_path)\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        print('Start predicting ......')\n",
    "        if not is_recursive:\n",
    "            predictions, images, labels, imagenames = sess.run([decoded, images_sh, labels_sh, imagenames_sh])\n",
    "            imagenames = np.reshape(imagenames, newshape=imagenames.shape[0])\n",
    "            imagenames = [tmp.decode('utf-8') for tmp in imagenames]\n",
    "            preds_res = decoder.sparse_tensor_to_str(predictions[0])\n",
    "            gt_res = decoder.sparse_tensor_to_str(labels)\n",
    "\n",
    "            accuracy = []\n",
    "\n",
    "            for index, gt_label in enumerate(gt_res):\n",
    "                pred = preds_res[index]\n",
    "                totol_count = len(gt_label)\n",
    "                correct_count = 0\n",
    "                try:\n",
    "                    for i, tmp in enumerate(gt_label):\n",
    "                        if tmp == pred[i]:\n",
    "                            correct_count += 1\n",
    "                except IndexError:\n",
    "                    continue\n",
    "                finally:\n",
    "                    try:\n",
    "                        accuracy.append(correct_count / totol_count)\n",
    "                    except ZeroDivisionError:\n",
    "                        if len(pred) == 0:\n",
    "                            accuracy.append(1)\n",
    "                        else:\n",
    "                            accuracy.append(0)\n",
    "\n",
    "            accuracy = np.mean(np.array(accuracy).astype(np.float32), axis=0)\n",
    "            print('Mean test accuracy is {:5f}'.format(accuracy))\n",
    "\n",
    "            for index, image in enumerate(images):\n",
    "                print('Predict {:s} image with gt label: {:s} **** predict label: {:s}'.format(\n",
    "                    imagenames[index], gt_res[index], preds_res[index]))\n",
    "                if is_vis:\n",
    "                    plt.imshow(image[:, :, (2, 1, 0)])\n",
    "                    plt.show()\n",
    "        else:\n",
    "            accuracy = []\n",
    "            for epoch in range(loops_nums):\n",
    "                predictions, images, labels, imagenames = sess.run([decoded, images_sh, labels_sh, imagenames_sh])\n",
    "                imagenames = np.reshape(imagenames, newshape=imagenames.shape[0])\n",
    "                imagenames = [tmp.decode('utf-8') for tmp in imagenames]\n",
    "                preds_res = decoder.sparse_tensor_to_str(predictions[0])\n",
    "                gt_res = decoder.sparse_tensor_to_str(labels)\n",
    "\n",
    "                for index, gt_label in enumerate(gt_res):\n",
    "                    pred = preds_res[index]\n",
    "                    totol_count = len(gt_label)\n",
    "                    correct_count = 0\n",
    "                    try:\n",
    "                        for i, tmp in enumerate(gt_label):\n",
    "                            if tmp == pred[i]:\n",
    "                                correct_count += 1\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                    finally:\n",
    "                        try:\n",
    "                            accuracy.append(correct_count / totol_count)\n",
    "                        except ZeroDivisionError:\n",
    "                            if len(pred) == 0:\n",
    "                                accuracy.append(1)\n",
    "                            else:\n",
    "                                accuracy.append(0)\n",
    "\n",
    "                for index, image in enumerate(images):\n",
    "                    print('Predict {:s} image with gt label: {:s} **** predict label: {:s}'.format(\n",
    "                        imagenames[index], gt_res[index], preds_res[index]))\n",
    "                    # if is_vis:\n",
    "                    #     plt.imshow(image[:, :, (2, 1, 0)])\n",
    "                    #     plt.show()\n",
    "\n",
    "            accuracy = np.mean(np.array(accuracy).astype(np.float32), axis=0)\n",
    "            print('Test accuracy is {:5f}'.format(accuracy))\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads=threads)\n",
    "\n",
    "    nonzero_weights = 0\n",
    "    total_weights = 0\n",
    "    for layer in tf.trainable_variables():\n",
    "            layer_weights = sess.run(layer)\n",
    "            #print(layer.name, np.count_nonzero(layer_weights)/np.prod(layer_weights.shape))\n",
    "            nonzero_weights += np.count_nonzero(layer_weights)\n",
    "            total_weights += np.prod(layer_weights.shape)\n",
    "    print(\"Reduced to %f percent of its original size.\" % (100*(nonzero_weights/total_weights)))\n",
    "    sess.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/sf_Shared/ECE-209AS/Final Project/AIonChip_HOZ/sandbox/Deeplearning-OCR/local_utils/data_utils.py:241: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/hamza/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/hamza/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/hamza/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/hamza/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/hamza/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /media/sf_Shared/ECE-209AS/Final Project/AIonChip_HOZ/sandbox/Deeplearning-OCR/local_utils/data_utils.py:242: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
      "WARNING:tensorflow:From <ipython-input-2-fb0f247679ec>:12: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From /home/hamza/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/sparse_ops.py:2670: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /media/sf_Shared/ECE-209AS/Final Project/AIonChip_HOZ/sandbox/Deeplearning-OCR/crnn_model/crnn_model.py:121: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/hamza/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/rnn/python/ops/rnn.py:233: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/hamza/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From <ipython-input-2-fb0f247679ec>:35: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "WARNING:tensorflow:From /home/hamza/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The passed save_path is not a valid checkpoint: model/shadownet/checkpoints/5_prune_25_iter_50_epochs/model_pre_pruned.ckpt-28",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-698ac8a2f87a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:s} doesn\\'t exist'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_shadownet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-fb0f247679ec>\u001b[0m in \u001b[0;36mtest_shadownet\u001b[0;34m(dataset_dir, weights_path, is_vis, is_recursive)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# restore the model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \"\n\u001b[0;32m-> 1268\u001b[0;31m                        + compat.as_text(save_path))\n\u001b[0m\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The passed save_path is not a valid checkpoint: model/shadownet/checkpoints/5_prune_25_iter_50_epochs/model_pre_pruned.ckpt-28"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"dataset/old_only\"\n",
    "#weights_path = \"model/shadownet/shadownet_2019-02-14-03-49-38.ckpt-6076\"\n",
    "weights_path = \"model/shadownet/checkpoints/5_prune_25_iter_50_epochs/model_pre_pruned.ckpt-28\"\n",
    "if not ops.exists(dataset_dir):\n",
    "    raise ValueError('{:s} doesn\\'t exist'.format(dataset_dir))\n",
    "test_shadownet(dataset_dir, weights_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
